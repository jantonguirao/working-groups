<!--- SPDX-License-Identifier: Apache-2.0 -->

# Wed Dec 8, 2021 at 8:00am PDT

## Agenda
* ONNX Compose utils - Status update
* Batch processing with a Loop

## Atendees
* Joaquin Anton (NVIDIA)
* Rodolfo Gabriel Esteves (Intel)
* Ganesan Ramalingan (Microsoft)

## Recording & Slides

* [Meeting Recording](https://lists.lfaidata.foundation/g/onnx-wg-preprocessing/files/onnx_preprocessing_20211208.mp4)
* [Meeting Slides](slides/20211208_slides.pdf)

## Meeting notes:

* ONNX compose utils has been merged. It allow users to combine two models by connecting inputs/outputs, optionally renaming entries in each graph.
* Joaquin showed some early results for batch preprocessing with a loop. The proposal is to create a "preprocessing" function that applies a subgraph to each sample in the graph, concatenating the results to a single tensor at the end.
* Rama suggests that we divide it into two steps. The first one "SequenceMap" or similar, applies a graph to each sample in a sequence, producing a sequence as an output. The second step, "ConcatFromSequence" already exists. The group agreed with this proposal.
* We discussed how to keep the preprocessing part of the graph identifiable within a combined model.
* We agreed to group the preprocessing part of the graph as a local function in the model, and use model level properties to identify a preprocessing function when applicable.
* We discussed a possibility of using a preprocessing function that would allow to "select" from different implementations at runtime (e.g. no preprocessing, inference pipeline, training pipeline). We agree that this is doable with the current proposal, but we do not need to standardize such usage at this moment.

## Action items:
* Joaquin will prepare a draft for the "SequenceMap" function and create an example that uses it, together with a local function tagged as the preprocessing function with model level properties.